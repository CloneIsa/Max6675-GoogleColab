{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIjq9AhumuTq0a4h5mu3uX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CloneIsa/Max6675-GoogleColab/blob/main/Max6675%26MQTT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando a SDK do Google\n"
      ],
      "metadata": {
        "id": "lW48KctqJobU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RioeYWrrDl9j"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurando a minha API Key"
      ],
      "metadata": {
        "id": "J42o_-RbJ0pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('SECRET_KEY')\n",
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "73niJtM0Jug6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listar os modelos disponíveis"
      ],
      "metadata": {
        "id": "H9ZOXvODKCZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "9NQd1yBNJ4K-",
        "outputId": "c76e4a7d-f958-487c-ff57-056b81adeca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurando os parâmetros de temperatura (Criatividad da IA)"
      ],
      "metadata": {
        "id": "Rdt6csOjNLCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = {\n",
        "    \"candidate_count\": 1,\n",
        "    \"temperature\": 0.5,\n",
        "}"
      ],
      "metadata": {
        "id": "XcI-SD7_MfqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurações de segurança"
      ],
      "metadata": {
        "id": "LjUGRhXaNX6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "safety_settings = {\n",
        "    \"HARASSMENT\": \"BLOCK_NONE\",\n",
        "    \"HATE\": \"BLOCK_NONE\",\n",
        "    \"SEXUAL\": \"BLOCK_NONE\",\n",
        "    \"DANGEROUS\": \"BLOCK_NONE\"\n",
        "}"
      ],
      "metadata": {
        "id": "czmQtKn-Naqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializando o modelo - após digitar model e colocar o ponto final, clique Ctrl e espaço para visualizar a listagem"
      ],
      "metadata": {
        "id": "PPo5vGCYO4lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    model_name = 'gemini-1.0-pro',\n",
        "    generation_config = generation_config,\n",
        "    safety_settings=safety_settings)"
      ],
      "metadata": {
        "id": "mOkwPpgqO52D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validando a saída:"
      ],
      "metadata": {
        "id": "5tM7p-FKUvKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Vamos aprender conteúdos sobre IA. Me dê sugetões\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "Gic1S009Utol",
        "outputId": "79cf94cf-629d-4cc3-cba9-1a892eaba88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Conceitos Fundamentais:**\n",
            "\n",
            "* Definição de Inteligência Artificial (IA)\n",
            "* Tipos de IA: aprendizado de máquina, aprendizado profundo, aprendizado por reforço\n",
            "* Algoritmos de IA: árvores de decisão, redes neurais, máquinas de vetores de suporte\n",
            "* Dados e recursos para IA\n",
            "\n",
            "**Aplicações da IA:**\n",
            "\n",
            "* Visão computacional (reconhecimento de imagem, processamento de vídeo)\n",
            "* Processamento de linguagem natural (tradução, geração de texto)\n",
            "* Robótica e automação\n",
            "* Cuidados de saúde e medicina\n",
            "* Finanças e negócios\n",
            "\n",
            "**Desafios e Considerações Éticas:**\n",
            "\n",
            "* Viés e discriminação em IA\n",
            "* Privacidade e segurança de dados\n",
            "* Impacto no emprego e na sociedade\n",
            "* Regulamentações e políticas éticas para IA\n",
            "\n",
            "**Aprendizado de Máquina:**\n",
            "\n",
            "* Conceitos de aprendizado supervisionado, não supervisionado e por reforço\n",
            "* Modelos de aprendizado de máquina: regressão, classificação, agrupamento\n",
            "* Avaliação e otimização de modelos\n",
            "* Técnicas de pré-processamento e engenharia de recursos\n",
            "\n",
            "**Aprendizado Profundo:**\n",
            "\n",
            "* Redes neurais: convolucionais, recorrentes, transformadores\n",
            "* Arquiteturas de rede profunda: ResNet, Inception, BERT\n",
            "* Treinamento e ajuste de modelos de aprendizado profundo\n",
            "* Aplicações de aprendizado profundo em visão computacional, processamento de linguagem natural e outras áreas\n",
            "\n",
            "**Aprendizado por Reforço:**\n",
            "\n",
            "* Conceitos de ambientes, ações, recompensas e políticas\n",
            "* Algoritmos de aprendizado por reforço: Q-learning, SARSA, A2C\n",
            "* Aplicações de aprendizado por reforço em jogos, robótica e planejamento\n",
            "\n",
            "**Ferramentas e Bibliotecas:**\n",
            "\n",
            "* Bibliotecas de IA: TensorFlow, PyTorch, Keras\n",
            "* Ferramentas de desenvolvimento de IA: Jupyter Notebooks, AWS SageMaker, Azure Machine Learning\n",
            "* Recursos online e comunidades de IA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando o chat boot"
      ],
      "metadata": {
        "id": "4OzCx1J3U5xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(history=[])"
      ],
      "metadata": {
        "id": "028Jty6PU5AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Esperando o prompt: \")\n",
        "\n",
        "while prompt != \"fim\":\n",
        "  response = chat.send_message(prompt)\n",
        "  print(\"Resposta: \", response.text, \"\\n\")\n",
        "  prompt = input(\"Esperando o prompt: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "ILf618dgVlR_",
        "outputId": "e1dff7e1-a619-4c8b-c123-c47d6370d530"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esperando o prompt: Meu primo nasceu nessa cidade. Qual é a nacionalidade dele?\n",
            "Resposta:  Japonesa \n",
            "\n",
            "Esperando o prompt: Qual é a população dessa cidade?\n",
            "Resposta:  12,6 milhões (2023) \n",
            "\n",
            "Esperando o prompt: População metropolitana\n",
            "Resposta:  37,34 milhões (2023) \n",
            "\n",
            "Esperando o prompt: fim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Melhorando a visualização\n",
        "# Código disponível em https://ai.google.dev/tutorials/python_quickstart#import_packages\n",
        "\n",
        "import textwrap\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('``', '\"')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "# Imprimindo o historico\n",
        "for message in chat.history:\n",
        "  display(to_markdown(f\"**{message.role}**: {message.parts[0].text}\"))\n",
        "  print('-----------------------------------------')"
      ],
      "metadata": {
        "id": "F2UE4lQsbH_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MQTT e Max6675"
      ],
      "metadata": {
        "id": "-_y6mWVFDlij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando bibliotecas"
      ],
      "metadata": {
        "id": "dRmraJ1kTesm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install paho-mqtt\n",
        "!pip install --upgrade paho-mqtt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbN2BxyzORnd",
        "outputId": "026dc15b-2086-4d34-da5c-3d54c2c340b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: paho-mqtt in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: paho-mqtt in /usr/local/lib/python3.10/dist-packages (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código da leitura"
      ],
      "metadata": {
        "id": "X-sUf5LpThzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação das bibliotecas:\n",
        "# pip install paho-mqtt pandas\n",
        "\n",
        "import paho.mqtt.client as mqtt\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Define o endereço do broker MQTT\n",
        "mqttBroker = \"broker.hivemq.com\"\n",
        "mqttPort = 1883\n",
        "topic = \"tacho/temperatura\"\n",
        "\n",
        "# Variáveis globais para armazenar os dados\n",
        "temperaturas = []\n",
        "tempos = []\n",
        "\n",
        "# Função chamada quando a conexão com o broker é estabelecida\n",
        "def on_connect(client, userdata, flags, rc):\n",
        "  print(\"Conectado ao broker MQTT com código de resultado: \"+str(rc))\n",
        "  client.subscribe(topic)\n",
        "\n",
        "# Função chamada quando uma mensagem é recebida\n",
        "def on_message(client, userdata, message):\n",
        "  global temperaturas, tempos\n",
        "  temperatura = float(message.payload.decode(\"utf-8\"))\n",
        "  timestamp = datetime.now()\n",
        "  temperaturas.append(temperatura)\n",
        "  tempos.append(timestamp)\n",
        "  print(f\"[{timestamp}] Temperatura: {temperatura} °C\")\n",
        "\n",
        "# Função para salvar os dados em um arquivo\n",
        "def salvar_dados():\n",
        "  global temperaturas, tempos\n",
        "  nome_arquivo = input(\"Digite o nome do arquivo com a extensão desejada (ex: dados.csv, dados.txt): \")\n",
        "  df = pd.DataFrame({\"Tempo\": tempos, \"Temperatura\": temperaturas})\n",
        "\n",
        "  if nome_arquivo.endswith(\".csv\"):\n",
        "    df.to_csv(nome_arquivo, index=False)\n",
        "  elif nome_arquivo.endswith(\".txt\"):\n",
        "    with open(nome_arquivo, \"w\") as f:\n",
        "      for tempo, temperatura in zip(tempos, temperaturas):\n",
        "        f.write(f\"{tempo},{temperatura}\\n\")\n",
        "  else:\n",
        "    print(\"Formato de arquivo não suportado. Use .csv ou .txt.\")\n",
        "    return\n",
        "\n",
        "  print(f\"Dados salvos em {nome_arquivo}\")\n",
        "\n",
        "# Função para retornar a temperatura a 'minutos' minutos atrás\n",
        "def temperatura_minutos_atras(minutos):\n",
        "  global temperaturas, tempos\n",
        "  if len(tempos) == 0:\n",
        "    return \"Nenhum dado de temperatura disponível.\"\n",
        "  tempo_alvo = datetime.now() - pd.Timedelta(minutes=minutos)\n",
        "  for i in range(len(tempos) - 1, -1, -1):\n",
        "    if tempos[i] <= tempo_alvo:\n",
        "      return temperaturas[i]\n",
        "  return \"Nenhum dado encontrado para esse período.\"\n",
        "\n",
        "# Função para retornar a temperatura a 'horas' horas atrás\n",
        "def temperatura_horas_atras(horas):\n",
        "  global temperaturas, tempos\n",
        "  if len(tempos) == 0:\n",
        "    return \"Nenhum dado de temperatura disponível.\"\n",
        "  tempo_alvo = datetime.now() - pd.Timedelta(hours=horas)\n",
        "  for i in range(len(tempos) - 1, -1, -1):\n",
        "    if tempos[i] <= tempo_alvo:\n",
        "      return temperaturas[i]\n",
        "  return \"Nenhum dado encontrado para esse período.\"\n",
        "\n",
        "# Função para retornar a temperatura máxima\n",
        "def temperatura_maxima():\n",
        "  global temperaturas\n",
        "  if len(temperaturas) == 0:\n",
        "    return \"Nenhum dado de temperatura disponível.\"\n",
        "  return max(temperaturas)\n",
        "\n",
        "# Função para retornar a temperatura mínima\n",
        "def temperatura_minima():\n",
        "  global temperaturas\n",
        "  if len(temperaturas) == 0:\n",
        "    return \"Nenhum dado de temperatura disponível.\"\n",
        "  return min(temperaturas)\n",
        "\n",
        "# Função principal do chatbot\n",
        "def chatbot():\n",
        "  while True:\n",
        "    comando = input(\"Você deseja visualizar qual dado? (temperatura, max, min, salvar, sair): \")\n",
        "\n",
        "    if comando == \"temperatura\":\n",
        "      periodo = input(\"Digite o período de tempo (minutos ou horas): \")\n",
        "      if periodo == \"minutos\":\n",
        "        minutos = int(input(\"Digite o número de minutos: \"))\n",
        "        temperatura = temperatura_minutos_atras(minutos)\n",
        "        print(f\"Temperatura {minutos} minutos atrás: {temperatura} °C\")\n",
        "      elif periodo == \"horas\":\n",
        "        horas = int(input(\"Digite o número de horas: \"))\n",
        "        temperatura = temperatura_horas_atras(horas)\n",
        "        print(f\"Temperatura {horas} horas atrás: {temperatura} °C\")\n",
        "      else:\n",
        "        print(\"Período inválido. Use 'minutos' ou 'horas'.\")\n",
        "\n",
        "    elif comando == \"max\":\n",
        "      temperatura = temperatura_maxima()\n",
        "      print(f\"Temperatura máxima: {temperatura} °C\")\n",
        "\n",
        "    elif comando == \"min\":\n",
        "      temperatura = temperatura_minima()\n",
        "      print(f\"Temperatura mínima: {temperatura} °C\")\n",
        "\n",
        "    elif comando == \"salvar\":\n",
        "      salvar_dados()\n",
        "\n",
        "    elif comando == \"sair\":\n",
        "      break\n",
        "\n",
        "    else:\n",
        "      print(\"Comando inválido.\")\n",
        "\n",
        "# Cria o cliente MQTT\n",
        "client = mqtt.Client()\n",
        "\n",
        "# Define as funções de callback\n",
        "client.on_connect = on_connect\n",
        "client.on_message = on_message\n",
        "\n",
        "# Conecta ao broker MQTT\n",
        "client.connect(mqttBroker, mqttPort)\n",
        "\n",
        "# Inicia o loop de eventos em uma thread separada\n",
        "client.loop_start()\n",
        "\n",
        "# Inicia o chatbot\n",
        "chatbot()\n",
        "\n",
        "# Desconecta do broker MQTT\n",
        "client.loop_stop()\n",
        "client.disconnect()"
      ],
      "metadata": {
        "id": "dILH7U4NRIvi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}